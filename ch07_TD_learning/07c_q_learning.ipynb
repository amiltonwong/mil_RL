{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: q_learning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "from monte_carlo_es import max_dict\n",
    "from td0_prediction import random_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.10| -0.10| -0.10|  1.00| \n",
      "---------------------------\n",
      "-0.10|  0.00| -0.10| -1.00| \n",
      "---------------------------\n",
      "-0.10| -0.10| -0.10| -0.10| \n",
      "it: 0\n",
      "it: 2000\n",
      "it: 4000\n",
      "it: 6000\n",
      "it: 8000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFUVJREFUeJzt3X/wXXWd3/Hn23wlQRAIIVIgYMIQ7YSuLfoVcbvuOlIR\ntEvW2dBNdjvGlg72B7PdtR03jF1U6nYWZ1fs1kzXbMFStmtC0doMxmZQnHbHQsw3okCEwJeA8DWy\nfCEQBA0k8O4f90Rvbm5yzje5yf1+P+f5mLnzPedzPufe9+d7ktc933PPuScyE0lSO7xm2AVIko4d\nQ1+SWsTQl6QWMfQlqUUMfUlqEUNfklrE0JekFjH0JalFDH1JapGRYRfQ67TTTsuFCxcOuwxJmlG2\nbNnydGbOr+s37UJ/4cKFjI2NDbsMSZpRIuKHTfp5eEeSWsTQl6QWMfQlqUUMfUlqEUNfklqkUehH\nxKURsS0ixiNiVZ/lvxoR342IvRGxrGfZyoh4uHqsHFThkqSpqw39iJgFrAYuA5YAKyJiSU+3x4EP\nA3/Vs+6pwCeAdwAXAp+IiLlHXrYk6XA02dO/EBjPzO2Z+TKwFlja3SEzH8vMe4FXe9Z9H3BHZu7M\nzGeBO4BLB1D3AZ7ctZvla+7iwSefPxpPL0lFaBL6ZwFPdM1PVG1NNFo3Iq6KiLGIGJucnGz41Pu7\n5/FnuXv7Ti793F8f1vqS1AZNQj/6tDW9m3qjdTNzTWaOZubo/Pm1VxH3tWj+CYe1niS1SZPQnwDO\n7ppfAOxo+PxHsu6URN/3F0lStyahvxlYHBGLIuI4YDmwvuHzbwQuiYi51Qe4l1RtkqQhqA39zNwL\nXE0nrB8Abs3MrRFxXURcDhARb4+ICeAK4AsRsbVadyfw7+m8cWwGrqvaBi7c0ZekWo2+ZTMzNwAb\netqu7ZreTOfQTb91bwJuOoIaGzHzJameV+RKUosUE/oe3pGkesWEvgd4JKleQaEvSapTTOh7eEeS\n6pUT+sMuQJJmgGJCX5JUr5jQD4/vSFKtckJ/2AVI0gxQTOhLkuoVE/oe3ZGkeuWEvgd4JKlWMaEv\nSapXTOh7eEeS6hUT+pKkeoa+JLVIMaHv4R1JqldQ6Jv6klSnmNCXJNUrJvTdz5ekeuWEvqkvSbWK\nCX1JUr1iQt+vYZCkeuWEvpkvSbWKCX1JUr1iQt8dfUmqV0zom/qSVK+c0Jck1Som9D17R5LqlRP6\nZr4k1WoU+hFxaURsi4jxiFjVZ/nsiFhXLd8UEQur9tdGxM0RcV9EPBAR1wy2fEnSVNSGfkTMAlYD\nlwFLgBURsaSn25XAs5l5HnADcH3VfgUwOzN/CXgb8JF9bwiD5o6+JNVrsqd/ITCemdsz82VgLbC0\np89S4OZq+jbg4uh813ECJ0TECHA88DLw/EAq7+FXK0tSvSahfxbwRNf8RNXWt09m7gV2AfPovAG8\nCPwYeBz4k8zceYQ1S5IOU5PQ77cLnQ37XAi8ApwJLAL+TUSce8ALRFwVEWMRMTY5OdmgpGZFSpL2\n1yT0J4Czu+YXADsO1qc6lHMysBP4beB/Z+aezHwK+DYw2vsCmbkmM0czc3T+/PlTHwWevSNJTTQJ\n/c3A4ohYFBHHAcuB9T191gMrq+llwJ2ZmXQO6bwnOk4ALgIeHEzpkqSpqg396hj91cBG4AHg1szc\nGhHXRcTlVbcbgXkRMQ58FNh3Wudq4ETgfjpvHl/MzHsHPAbAi7MkqYmRJp0ycwOwoaft2q7p3XRO\nz+xd74V+7UeFmS9JtbwiV5JapJjQlyTVKyb03dGXpHrlhL7HdySpVjGhL0mqV0zou58vSfXKCX1T\nX5JqFRP6kqR6xYS+V+RKUr1yQt/Ml6RaxYR+t917Xhl2CZI0LRUZ+s/9dM+wS5CkaamY0O8+vJMH\n3ONFkgQFhb4kqV4xod999k66oy9JfZUT+p69I0m1ign9bu7oS1J/xYS+O/qSVK+c0I/uY/ru60tS\nP8WEfjczX5L6Kyb0PbwjSfXKCX1TX5JqFRP6kqR6xYS+98iVpHrFhL4kqV6Roe/ZO5LUX5GhL0nq\nr8jQ96uVJam/IkNfktSfoS9JLVJk6PtBriT11yj0I+LSiNgWEeMRsarP8tkRsa5avikiFnYte0tE\n3BURWyPivoiYM7jyJUlTURv6ETELWA1cBiwBVkTEkp5uVwLPZuZ5wA3A9dW6I8BfAv88M88H3g14\n13JJGpIme/oXAuOZuT0zXwbWAkt7+iwFbq6mbwMujs4lspcA92bm9wEy85nMfGUwpUuSpqpJ6J8F\nPNE1P1G19e2TmXuBXcA84E1ARsTGiPhuRHys3wtExFURMRYRY5OTk1MdgySpoSah3+9LbXo/Kj1Y\nnxHgV4DfqX5+MCIuPqBj5prMHM3M0fnz5zco6dD8HFeS+msS+hPA2V3zC4AdB+tTHcc/GdhZtf+f\nzHw6M38KbADeeqRFS5IOT5PQ3wwsjohFEXEcsBxY39NnPbCyml4G3JmdexZuBN4SEa+r3gx+DfjB\nYEo/uB8+8+LRfglJmpFqQ786Rn81nQB/ALg1M7dGxHURcXnV7UZgXkSMAx8FVlXrPgt8ls4bx/eA\n72bm1wY/jP19+Iubj/ZLSNKMNNKkU2ZuoHNoprvt2q7p3cAVB1n3L+mctilJGrIir8iVJPVn6EtS\nixj6ktQihr4ktYihL0ktYuhLUosY+pLUIoa+JLWIoS9JLVJs6Kf3TJSkAxQb+k8+v3vYJUjStFNs\n6LujL0kHKjb0JUkHKjb03dGXpAMVG/qSpAMZ+pLUIsWGvqdsStKBig19SdKBDH1JapFiQ/+ZF14e\ndgmSNO0UG/pLV3/b4/qS1KPY0JckHcjQl6QWMfQlqUUMfUlqkaJD389xJWl/RYe+JGl/hr4ktYih\nL0ktUnToP/ezPcMuQZKmlUahHxGXRsS2iBiPiFV9ls+OiHXV8k0RsbBn+TkR8UJE/NvBlN3Mdx59\n5li+nCRNe7WhHxGzgNXAZcASYEVELOnpdiXwbGaeB9wAXN+z/Abg60deriTpSDTZ078QGM/M7Zn5\nMrAWWNrTZylwczV9G3BxRARARPwGsB3YOpiSJUmHq0nonwU80TU/UbX17ZOZe4FdwLyIOAH4A+BT\nR16qJOlINQn96NPWe9nTwfp8CrghM1845AtEXBURYxExNjk52aAkSdLhGGnQZwI4u2t+AbDjIH0m\nImIEOBnYCbwDWBYRnwFOAV6NiN2Z+fnulTNzDbAGYHR0dGDX0XpFriTtr0nobwYWR8Qi4EfAcuC3\ne/qsB1YCdwHLgDuz82X279rXISI+CbzQG/iSpGOnNvQzc29EXA1sBGYBN2Xm1oi4DhjLzPXAjcAt\nETFOZw9/+dEsuqnde18ZdgmSNK002dMnMzcAG3raru2a3g1cUfMcnzyM+o7IZ+94iA9esOBYv6wk\nTVtFX5G766dekStJ3YoOfT/HlaT9lR36pr4k7afo0Jck7a/o0E939SVpP0WHviRpf0WH/osve56+\nJHUrOvQlSfsz9CWpRQx9SWoRQ1+SWsTQl6QWKT70X33Vc/UlaZ+iQn/kNQfewOu/3fXYMa9Dkqar\nokJ/1WV/+4C2x3f+bAiVSNL0VFToS5IOzdCXpBYpPvTTb9WXpJ8rPvS9e5Yk/UJRoX/7vT8+oO0r\n9/xoCJVI0vRUVOg/uWv3sEuQpGmtqNAfmXXgefqSpF8oK/T7XJwlSfqFokI/wtCXpEMpKvQlSYdW\nVOi7oy9Jh1ZW6A+7AEma5soKfXf1JemQigp9T96RpEMrKvTf/LdOGnYJkjStFRX6H7zgzGGXIEnT\nWqPQj4hLI2JbRIxHxKo+y2dHxLpq+aaIWFi1vzcitkTEfdXP9wy2/J46/ChXkg6pNvQjYhawGrgM\nWAKsiIglPd2uBJ7NzPOAG4Drq/angV/PzF8CVgK3DKpwSdLUNdnTvxAYz8ztmfkysBZY2tNnKXBz\nNX0bcHFERGbek5k7qvatwJyImD2Iwvs55XWv7dv+H7/x8NF6SUmaUZqE/lnAE13zE1Vb3z6ZuRfY\nBczr6fObwD2Z+VLvC0TEVRExFhFjk5OTTWs/wAXnzO3bfsM3Hjrs55SkkjQJ/X4HyntvR3XIPhFx\nPp1DPh/p9wKZuSYzRzNzdP78+Q1KkiQdjiahPwGc3TW/ANhxsD4RMQKcDOys5hcA/xP4UGY+cqQF\nS5IOX5PQ3wwsjohFEXEcsBxY39NnPZ0PagGWAXdmZkbEKcDXgGsy89uDKvpwrNv8+DBfXpKmhdrQ\nr47RXw1sBB4Abs3MrRFxXURcXnW7EZgXEePAR4F9p3VeDZwH/GFEfK96vGHgo2jg07c/MIyXlaRp\nZaRJp8zcAGzoabu2a3o3cEWf9T4NfPoIa5QkDUhRV+Qeyk9e2svLe18ddhmSNFStCX2AP71j27BL\nkKShalXo/+jZnw27BEkaqlaFfvZeXSBJLdOq0H/lVVNfUru1KvQlqe1aFfr3Tjw37BIkaahaFfo7\ndu0edgmSNFStCn1JarvWhf6un+0ZdgmSNDStC/2H/uYnwy5BkoamdaH/fx86/Ju0SNJMV1zo//rf\nPfOQy//TnePHqBJJmn6KC/3TX3/UbsErSTNecaF/0vH9b44uSSow9N+1+LTaPq/6dQySWqq40H/t\nrPohbfMMHkktVVzon3/mSbV9fm/t945BJZI0/RQX+hFR28c9fUltVVzoS5IOrrWhv/77O4ZdgiQd\nc60N/d/90j3DLkGSjrnWhr4ktVGrQ/8ffeEudjznzdIltUerQ/87j+7kl//4Tp563purSGqHVof+\nPhf+h2/67ZuSWsHQr/zuWj/YlVS+IkP/I7927pTXee6ne1ix5u6jUI0kTR9Fhv5V75p66APctf0Z\nJn/y0oCrkaTpo8jQn3fi4X+n/tv/6BvccvcPefGlvQOsSJKmh0ahHxGXRsS2iBiPiFV9ls+OiHXV\n8k0RsbBr2TVV+7aIeN/gSj96/vCr93P+JzZy1yPPsG7z4+x55dVhlyRJAzFS1yEiZgGrgfcCE8Dm\niFifmT/o6nYl8GxmnhcRy4Hrgd+KiCXAcuB84EzgGxHxpsx8ZdADORpW/EXnGP8ffPk+AJa9bQG/\n/9438fo5I5w0x5u1SJp5akMfuBAYz8ztABGxFlgKdIf+UuCT1fRtwOej83WXS4G1mfkS8GhEjFfP\nd9dgyj+4z/zmW/jYl+8d6HPetmWC27ZM/Hz+07/xd/jHF72R3Xte4bfW3M3b3ziXxaefyDsWzePp\nF15i0WkncPxxs5gzMouIZt8AKklHU5PQPwt4omt+AnjHwfpk5t6I2AXMq9rv7ln3rMOudgqWvW3B\nwEO/17/76v381//3GONPvQDA9594bsrPcd4bTsS3AkkA737zfD7+gSVH9TWahH6/TOq93+DB+jRZ\nl4i4CrgK4JxzzmlQUr3XvCZ47I8/wB997Qf8xV8/OpDn7HXSnBHefPrrOXvu8Xxr2+Fd3LX4DSfi\nHwCSAE4/ac5Rf40moT8BnN01vwDo/V7ifX0mImIEOBnY2XBdMnMNsAZgdHR0oDew/fgHlhz1d05J\nmimanL2zGVgcEYsi4jg6H8yu7+mzHlhZTS8D7szMrNqXV2f3LAIWA98ZTOmSpKmq3dOvjtFfDWwE\nZgE3ZebWiLgOGMvM9cCNwC3VB7U76bwxUPW7lc6HvnuBfzVTztyRpBJFZ4d8+hgdHc2xsbFhlyFJ\nM0pEbMnM0bp+RV6RK0nqz9CXpBYx9CWpRQx9SWoRQ1+SWmTanb0TEZPAD4/gKU4Dnh5QOTNB28YL\njrktHPPUvDEz59d1mnahf6QiYqzJaUulaNt4wTG3hWM+Ojy8I0ktYuhLUouUGPprhl3AMda28YJj\nbgvHfBQUd0xfknRwJe7pS5IOopjQr7t5+0wSEWdHxLci4oGI2BoR/7pqPzUi7oiIh6ufc6v2iIg/\nq8Z+b0S8teu5Vlb9H46IlQd7zekgImZFxD0RcXs1vygiNlW1r6u+2pvqq7rXVePdFBELu57jmqp9\nW0S8bzgjaSYiTomI2yLiwWpbv7MF2/j3q3/T90fElyJiTmnbOSJuioinIuL+rraBbdeIeFtE3Fet\n82cRU7wNU2bO+Aedr3x+BDgXOA74PrBk2HUdwXjOAN5aTb8eeAhYAnwGWFW1rwKur6bfD3ydzp3K\nLgI2Ve2nAturn3Or6bnDHt8hxv1R4K+A26v5W4Hl1fSfA/+imv6XwJ9X08uBddX0kmrbzwYWVf8m\nZg17XIcY783AP6umjwNOKXkb07lV6qPA8V3b98OlbWfgV4G3Avd3tQ1su9K5J8k7q3W+Dlw2pfqG\n/Qsa0C/5ncDGrvlrgGuGXdcAx/e/gPcC24AzqrYzgG3V9BeAFV39t1XLVwBf6Grfr990etC5q9o3\ngfcAt1f/oJ8GRnq3MZ17O7yzmh6p+kXvdu/uN90ewElVAEZPe8nbeN+9tE+tttvtwPtK3M7Awp7Q\nH8h2rZY92NW+X78mj1IO7/S7efsxuQH70Vb9SXsBsAk4PTN/DFD9fEPV7WDjn0m/l88BHwNerebn\nAc9l5t5qvrv2n4+rWr6r6j+TxnsuMAl8sTqk9V8i4gQK3saZ+SPgT4DHgR/T2W5bKHs77zOo7XpW\nNd3b3lgpod/oBuwzTUScCHwZ+L3MfP5QXfu0Nb4x/bBFxD8EnsrMLd3NfbpmzbIZMd7KCJ1DAP85\nMy8AXqTzZ//BzPgxV8exl9I5JHMmcAJwWZ+uJW3nOlMd4xGPvZTQb3QD9pkkIl5LJ/D/e2Z+pWr+\nm4g4o1p+BvBU1X6w8c+U38vfBy6PiMeAtXQO8XwOOCUi9t3Ss7v2n4+rWn4yndt0zpTxQqfWiczc\nVM3fRudNoNRtDPAPgEczczIz9wBfAX6ZsrfzPoParhPVdG97Y6WEfpObt88Y1afxNwIPZOZnuxZ1\n34B+JZ1j/fvaP1SdCXARsKv6E3IjcElEzK32si6p2qaVzLwmMxdk5kI62+7OzPwd4FvAsqpb73j3\n/R6WVf2zal9enfWxCFhM50OvaScznwSeiIg3V00X07mXdJHbuPI4cFFEvK76N75vzMVu5y4D2a7V\nsp9ExEXV7/BDXc/VzLA/8BjgByfvp3OWyyPAx4ddzxGO5Vfo/Ml2L/C96vF+Osczvwk8XP08teof\nwOpq7PcBo13P9U+B8erxT4Y9tgZjfze/OHvnXDr/mceB/wHMrtrnVPPj1fJzu9b/ePV72MYUz2oY\nwlj/HjBWbeev0jlLo+htDHwKeBC4H7iFzhk4RW1n4Et0PrPYQ2fP/MpBbldgtPr9PQJ8np6TAeoe\nXpErSS1SyuEdSVIDhr4ktYihL0ktYuhLUosY+pLUIoa+JLWIoS9JLWLoS1KL/H820NzNzSLOtAAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4945f12b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update counts:\n",
      "---------------------------\n",
      " 0.26|  0.05|  0.04|  0.00| \n",
      "---------------------------\n",
      " 0.12|  0.00|  0.01|  0.00| \n",
      "---------------------------\n",
      " 0.28|  0.07|  0.05|  0.11| \n",
      "values:\n",
      "---------------------------\n",
      " 0.62|  0.80|  1.00|  0.00| \n",
      "---------------------------\n",
      " 0.46|  0.00|  0.80|  0.00| \n",
      "---------------------------\n",
      " 0.31|  0.46|  0.62|  0.46| \n",
      "policy:\n",
      "---------------------------\n",
      "  R  |   R  |   R  |      | \n",
      "---------------------------\n",
      "  U  |      |   U  |      | \n",
      "---------------------------\n",
      "  U  |   R  |   U  |   L  | \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # NOTE: if we use the standard grid, there's a good chance we will end up with\n",
    "    # suboptimal policies\n",
    "    # e.g.\n",
    "    # ---------------------------\n",
    "    #   R  |   R  |   R  |      |\n",
    "    # ---------------------------\n",
    "    #   R* |      |   U  |      |\n",
    "    # ---------------------------\n",
    "    #   U  |   R  |   U  |   L  |\n",
    "    # since going R at (1,0) (shown with a *) incurs no cost, it's OK to keep doing that.\n",
    "    # we'll either end up staying in the same spot, or back to the start (2,0), at which\n",
    "    # point we whould then just go back up, or at (0,0), at which point we can continue\n",
    "    # on right.\n",
    "    # instead, let's penalize each movement so the agent will find a shorter route.\n",
    "    #\n",
    "    # grid = standard_grid()\n",
    "    grid = negative_grid(step_cost=-0.1)\n",
    "\n",
    "    # print rewards\n",
    "    print \"rewards:\"\n",
    "    print_values(grid.rewards, grid)\n",
    "\n",
    "    # no policy initialization, we will derive our policy from most recent Q\n",
    "\n",
    "    # initialize Q(s,a)\n",
    "    Q = {}\n",
    "    states = grid.all_states()\n",
    "    for s in states:\n",
    "        Q[s] = {}\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "            Q[s][a] = 0\n",
    "\n",
    "    # let's also keep track of how many times Q[s] has been updated\n",
    "    update_counts = {}\n",
    "    update_counts_sa = {}\n",
    "    for s in states:\n",
    "        update_counts_sa[s] = {}\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "            update_counts_sa[s][a] = 1.0\n",
    "\n",
    "    # repeat until convergence\n",
    "    t = 1.0\n",
    "    deltas = []\n",
    "    for it in xrange(10000):\n",
    "        if it % 100 == 0:\n",
    "            t += 1e-2\n",
    "        if it % 2000 == 0:\n",
    "            print \"it:\", it\n",
    "\n",
    "        # instead of 'generating' an epsiode, we will PLAY\n",
    "        # an episode within this loop\n",
    "        s = (2, 0) # start state\n",
    "        grid.set_state(s)\n",
    "\n",
    "        # the first (s, r) tuple is the state we start in and 0\n",
    "        # (since we don't get a reward) for simply starting the game\n",
    "        # the last (s, r) tuple is the terminal state and the final reward\n",
    "        # the value for the terminal state is by definition 0, so we don't\n",
    "        # care about updating it.\n",
    "        a, _ = max_dict(Q[s])\n",
    "        biggest_change = 0\n",
    "        while not grid.game_over():\n",
    "            a = random_action(a, eps=0.5/t) # epsilon-greedy\n",
    "            # random action also works, but slower since you can bump into walls\n",
    "            # a = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "            r = grid.move(a)\n",
    "            s2 = grid.current_state()\n",
    "\n",
    "            # adaptive learning rate\n",
    "            alpha = ALPHA / update_counts_sa[s][a]\n",
    "            update_counts_sa[s][a] += 0.005\n",
    "\n",
    "            # we will update Q(s,a) AS we experience the episode\n",
    "            old_qsa = Q[s][a]\n",
    "            # the difference between SARSA and Q-Learning is with Q-Learning\n",
    "            # we will use this max[a']{ Q(s',a')} in our update\n",
    "            # even if we do not end up taking this action in the next step\n",
    "            a2, max_q_s2a2 = max_dict(Q[s2])\n",
    "            Q[s][a] = Q[s][a] + alpha*(r + GAMMA*max_q_s2a2 - Q[s][a])\n",
    "            biggest_change = max(biggest_change, np.abs(old_qsa - Q[s][a]))\n",
    "\n",
    "            # we would like to know how often Q(s) has been updated too\n",
    "            update_counts[s] = update_counts.get(s,0) + 1\n",
    "\n",
    "            # next state becomes current state\n",
    "            s = s2\n",
    "         \n",
    "        deltas.append(biggest_change)\n",
    "\n",
    "    plt.plot(deltas)\n",
    "    plt.show()\n",
    "\n",
    "    # determine the policy from Q*\n",
    "    # find V* from Q*\n",
    "    policy = {}\n",
    "    V = {}\n",
    "    for s in grid.actions.keys():\n",
    "        a, max_q = max_dict(Q[s])\n",
    "        policy[s] = a\n",
    "        V[s] = max_q\n",
    "\n",
    "    # what's the proportion of time we spend updating each part of Q?\n",
    "    print \"update counts:\"\n",
    "    total = np.sum(update_counts.values())\n",
    "    for k, v in update_counts.iteritems():\n",
    "        update_counts[k] = float(v) / total\n",
    "    print_values(update_counts, grid)\n",
    "\n",
    "    print \"values:\"\n",
    "    print_values(V, grid)\n",
    "    print \"policy:\"\n",
    "    print_policy(policy, grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
